{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "word2vec_practical.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanghorui/samplecode/blob/master/word2vec_practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qleuKFvYED1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4i5Vik6bVcx",
        "colab_type": "code",
        "outputId": "63e9d2be-e580-45be-8ddc-6b09de52ba96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGiqzFuTYED9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv(\"/content/drive/My Drive/ML_datasets/word2vec-nlp-tutorial/labeledTrainData.tsv\", \n",
        "                    header=0, delimiter=\"\\t\",quoting=3)\n",
        "test = pd.read_csv(\"/content/drive/My Drive/ML_datasets/word2vec-nlp-tutorial/testData.tsv\", \n",
        "                    header=0, delimiter=\"\\t\",quoting=3)\n",
        "unlabeled_train = pd.read_csv(\"/content/drive/My Drive/ML_datasets/word2vec-nlp-tutorial/unlabeledTrainData.tsv\", \n",
        "                    header=0, delimiter=\"\\t\",quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRzw4T-_YEED",
        "colab_type": "code",
        "outputId": "b395b7df-e58b-402b-94f7-0a829dbe0306",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Read %d labeled train reviews, %d labeled test reviews, and %d unlabeled reviews\" \n",
        "      % (train[\"review\"].size, test[\"review\"].size, unlabeled_train[\"review\"].size ))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 25000 labeled train reviews, 25000 labeled test reviews, and 50000 unlabeled reviews\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfEZ0dE6YEEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import various modules for string cleaning\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def review_to_wordlist( review, remove_stopwords=False ):\n",
        "    # Function to convert a document to a sequence of words,\n",
        "    # optionally removing stop words.  Returns a list of words.\n",
        "    \n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "      \n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    \n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "    \n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "    \n",
        "    # 5. Return a list of words\n",
        "    return(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqd_WtWZYEEN",
        "colab_type": "code",
        "outputId": "4a48391e-af2c-4d86-c207-4b9922fff44d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk.data\n",
        "nltk.download('punkt') \n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "    # Function to split a review into parsed sentences. Returns a \n",
        "    # list of sentences, where each sentence is a list of words\n",
        "    \n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    \n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append(review_to_wordlist(raw_sentence,remove_stopwords ))\n",
        "    \n",
        "    # Return the list of sentences (each sentence is a list of words,\n",
        "    # so this returns a list of lists\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm51EHWeYEER",
        "colab_type": "code",
        "outputId": "71111c54-5647-40c3-ac9f-b46838fd03ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "print(\"Parsing sentences from training set\")\n",
        "for review in train[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n",
        "\n",
        "print(\"Parsing sentences from unlabeled set\")\n",
        "for review in unlabeled_train[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from unlabeled set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06VWXmS3YEEX",
        "colab_type": "code",
        "outputId": "7e0252e3-09e7-41b3-fe36-885624237f56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "795538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuuYoceQxARS",
        "colab_type": "code",
        "outputId": "1bfa6e18-9df6-49ad-901a-dbe11a13a624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(sentences[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['with', 'all', 'this', 'stuff', 'going', 'down', 'at', 'the', 'moment', 'with', 'mj', 'i', 've', 'started', 'listening', 'to', 'his', 'music', 'watching', 'the', 'odd', 'documentary', 'here', 'and', 'there', 'watched', 'the', 'wiz', 'and', 'watched', 'moonwalker', 'again']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3uFk80hyDii",
        "colab_type": "code",
        "outputId": "0b1c3060-0251-4067-fca4-f3ef4c854a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Import the built-in logging module and configure it so that Word2Vec \n",
        "# creates nice output messages\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "    level=logging.INFO)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 300    # Word vector dimensionality                      \n",
        "min_word_count = 40   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "print(\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
        "\n",
        "# If you don't plan to train the model any further, calling \n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and \n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "# model_name = \"300features_40minwords_10context\"\n",
        "model.save(\"/content/drive/My Drive/ML_datasets/300features_40minwords_10context\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-10 14:11:43,194 : INFO : 'pattern' package not found; tag filters are not available for English\n",
            "2020-01-10 14:11:43,204 : INFO : collecting all words and their counts\n",
            "2020-01-10 14:11:43,208 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2020-01-10 14:11:43,258 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
            "2020-01-10 14:11:43,314 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
            "2020-01-10 14:11:43,369 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020-01-10 14:11:43,427 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
            "2020-01-10 14:11:43,490 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
            "2020-01-10 14:11:43,550 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
            "2020-01-10 14:11:43,605 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
            "2020-01-10 14:11:43,661 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
            "2020-01-10 14:11:43,722 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
            "2020-01-10 14:11:43,782 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
            "2020-01-10 14:11:43,840 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
            "2020-01-10 14:11:43,899 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
            "2020-01-10 14:11:43,959 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
            "2020-01-10 14:11:44,014 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
            "2020-01-10 14:11:44,074 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
            "2020-01-10 14:11:44,144 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
            "2020-01-10 14:11:44,206 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
            "2020-01-10 14:11:44,266 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
            "2020-01-10 14:11:44,323 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
            "2020-01-10 14:11:44,383 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
            "2020-01-10 14:11:44,442 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
            "2020-01-10 14:11:44,501 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
            "2020-01-10 14:11:44,560 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
            "2020-01-10 14:11:44,623 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
            "2020-01-10 14:11:44,678 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
            "2020-01-10 14:11:44,735 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
            "2020-01-10 14:11:44,796 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types\n",
            "2020-01-10 14:11:44,857 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types\n",
            "2020-01-10 14:11:44,915 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types\n",
            "2020-01-10 14:11:44,974 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types\n",
            "2020-01-10 14:11:45,037 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types\n",
            "2020-01-10 14:11:45,096 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types\n",
            "2020-01-10 14:11:45,154 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types\n",
            "2020-01-10 14:11:45,214 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types\n",
            "2020-01-10 14:11:45,274 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types\n",
            "2020-01-10 14:11:45,333 : INFO : PROGRESS: at sentence #360000, processed 8019427 words, keeping 86596 word types\n",
            "2020-01-10 14:11:45,396 : INFO : PROGRESS: at sentence #370000, processed 8246619 words, keeping 87708 word types\n",
            "2020-01-10 14:11:45,456 : INFO : PROGRESS: at sentence #380000, processed 8471766 words, keeping 88878 word types\n",
            "2020-01-10 14:11:45,518 : INFO : PROGRESS: at sentence #390000, processed 8701497 words, keeping 89907 word types\n",
            "2020-01-10 14:11:45,577 : INFO : PROGRESS: at sentence #400000, processed 8924446 words, keeping 90916 word types\n",
            "2020-01-10 14:11:45,636 : INFO : PROGRESS: at sentence #410000, processed 9145796 words, keeping 91880 word types\n",
            "2020-01-10 14:11:45,700 : INFO : PROGRESS: at sentence #420000, processed 9366876 words, keeping 92912 word types\n",
            "2020-01-10 14:11:45,764 : INFO : PROGRESS: at sentence #430000, processed 9594413 words, keeping 93932 word types\n",
            "2020-01-10 14:11:45,826 : INFO : PROGRESS: at sentence #440000, processed 9821166 words, keeping 94906 word types\n",
            "2020-01-10 14:11:45,886 : INFO : PROGRESS: at sentence #450000, processed 10044928 words, keeping 96036 word types\n",
            "2020-01-10 14:11:45,948 : INFO : PROGRESS: at sentence #460000, processed 10277688 words, keeping 97088 word types\n",
            "2020-01-10 14:11:46,010 : INFO : PROGRESS: at sentence #470000, processed 10505613 words, keeping 97933 word types\n",
            "2020-01-10 14:11:46,071 : INFO : PROGRESS: at sentence #480000, processed 10725997 words, keeping 98862 word types\n",
            "2020-01-10 14:11:46,131 : INFO : PROGRESS: at sentence #490000, processed 10952741 words, keeping 99871 word types\n",
            "2020-01-10 14:11:46,189 : INFO : PROGRESS: at sentence #500000, processed 11174397 words, keeping 100765 word types\n",
            "2020-01-10 14:11:46,252 : INFO : PROGRESS: at sentence #510000, processed 11399672 words, keeping 101699 word types\n",
            "2020-01-10 14:11:46,312 : INFO : PROGRESS: at sentence #520000, processed 11623020 words, keeping 102598 word types\n",
            "2020-01-10 14:11:46,372 : INFO : PROGRESS: at sentence #530000, processed 11847418 words, keeping 103400 word types\n",
            "2020-01-10 14:11:46,432 : INFO : PROGRESS: at sentence #540000, processed 12072033 words, keeping 104265 word types\n",
            "2020-01-10 14:11:46,495 : INFO : PROGRESS: at sentence #550000, processed 12297571 words, keeping 105133 word types\n",
            "2020-01-10 14:11:46,556 : INFO : PROGRESS: at sentence #560000, processed 12518861 words, keeping 105997 word types\n",
            "2020-01-10 14:11:46,616 : INFO : PROGRESS: at sentence #570000, processed 12747916 words, keeping 106787 word types\n",
            "2020-01-10 14:11:46,677 : INFO : PROGRESS: at sentence #580000, processed 12969412 words, keeping 107665 word types\n",
            "2020-01-10 14:11:46,738 : INFO : PROGRESS: at sentence #590000, processed 13194937 words, keeping 108501 word types\n",
            "2020-01-10 14:11:46,798 : INFO : PROGRESS: at sentence #600000, processed 13417135 words, keeping 109218 word types\n",
            "2020-01-10 14:11:46,859 : INFO : PROGRESS: at sentence #610000, processed 13638158 words, keeping 110092 word types\n",
            "2020-01-10 14:11:46,919 : INFO : PROGRESS: at sentence #620000, processed 13864483 words, keeping 110837 word types\n",
            "2020-01-10 14:11:46,977 : INFO : PROGRESS: at sentence #630000, processed 14088769 words, keeping 111610 word types\n",
            "2020-01-10 14:11:47,043 : INFO : PROGRESS: at sentence #640000, processed 14309552 words, keeping 112416 word types\n",
            "2020-01-10 14:11:47,105 : INFO : PROGRESS: at sentence #650000, processed 14535308 words, keeping 113196 word types\n",
            "2020-01-10 14:11:47,165 : INFO : PROGRESS: at sentence #660000, processed 14758098 words, keeping 113945 word types\n",
            "2020-01-10 14:11:47,226 : INFO : PROGRESS: at sentence #670000, processed 14981482 words, keeping 114643 word types\n",
            "2020-01-10 14:11:47,285 : INFO : PROGRESS: at sentence #680000, processed 15206314 words, keeping 115354 word types\n",
            "2020-01-10 14:11:47,346 : INFO : PROGRESS: at sentence #690000, processed 15428507 words, keeping 116131 word types\n",
            "2020-01-10 14:11:47,407 : INFO : PROGRESS: at sentence #700000, processed 15657213 words, keeping 116943 word types\n",
            "2020-01-10 14:11:47,468 : INFO : PROGRESS: at sentence #710000, processed 15880202 words, keeping 117596 word types\n",
            "2020-01-10 14:11:47,529 : INFO : PROGRESS: at sentence #720000, processed 16105489 words, keeping 118221 word types\n",
            "2020-01-10 14:11:47,588 : INFO : PROGRESS: at sentence #730000, processed 16331870 words, keeping 118954 word types\n",
            "2020-01-10 14:11:47,648 : INFO : PROGRESS: at sentence #740000, processed 16552903 words, keeping 119668 word types\n",
            "2020-01-10 14:11:47,708 : INFO : PROGRESS: at sentence #750000, processed 16771230 words, keeping 120295 word types\n",
            "2020-01-10 14:11:47,766 : INFO : PROGRESS: at sentence #760000, processed 16990622 words, keeping 120930 word types\n",
            "2020-01-10 14:11:47,828 : INFO : PROGRESS: at sentence #770000, processed 17217759 words, keeping 121703 word types\n",
            "2020-01-10 14:11:47,891 : INFO : PROGRESS: at sentence #780000, processed 17447905 words, keeping 122402 word types\n",
            "2020-01-10 14:11:47,954 : INFO : PROGRESS: at sentence #790000, processed 17674981 words, keeping 123066 word types\n",
            "2020-01-10 14:11:47,989 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795538 sentences\n",
            "2020-01-10 14:11:47,990 : INFO : Loading a fresh vocabulary\n",
            "2020-01-10 14:11:48,068 : INFO : effective_min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
            "2020-01-10 14:11:48,069 : INFO : effective_min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)\n",
            "2020-01-10 14:11:48,127 : INFO : deleting the raw counts dictionary of 123504 items\n",
            "2020-01-10 14:11:48,133 : INFO : sample=0.001 downsamples 48 most-common words\n",
            "2020-01-10 14:11:48,136 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)\n",
            "2020-01-10 14:11:48,196 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
            "2020-01-10 14:11:48,197 : INFO : resetting layer weights\n",
            "2020-01-10 14:11:51,434 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2020-01-10 14:11:52,458 : INFO : EPOCH 1 - PROGRESS: at 2.56% examples, 324080 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:11:53,499 : INFO : EPOCH 1 - PROGRESS: at 5.14% examples, 320042 words/s, in_qsize 8, out_qsize 2\n",
            "2020-01-10 14:11:54,506 : INFO : EPOCH 1 - PROGRESS: at 7.88% examples, 326778 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:11:55,515 : INFO : EPOCH 1 - PROGRESS: at 10.64% examples, 331939 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:11:56,534 : INFO : EPOCH 1 - PROGRESS: at 13.25% examples, 330235 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:11:57,544 : INFO : EPOCH 1 - PROGRESS: at 15.89% examples, 330670 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:11:58,551 : INFO : EPOCH 1 - PROGRESS: at 18.58% examples, 331057 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:11:59,599 : INFO : EPOCH 1 - PROGRESS: at 21.28% examples, 330626 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:00,621 : INFO : EPOCH 1 - PROGRESS: at 23.98% examples, 331256 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:01,637 : INFO : EPOCH 1 - PROGRESS: at 26.57% examples, 330467 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:02,640 : INFO : EPOCH 1 - PROGRESS: at 29.20% examples, 330826 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:03,667 : INFO : EPOCH 1 - PROGRESS: at 31.85% examples, 329916 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:04,703 : INFO : EPOCH 1 - PROGRESS: at 34.54% examples, 329997 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-10 14:12:05,709 : INFO : EPOCH 1 - PROGRESS: at 37.17% examples, 330302 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:06,747 : INFO : EPOCH 1 - PROGRESS: at 39.86% examples, 330316 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:07,755 : INFO : EPOCH 1 - PROGRESS: at 42.52% examples, 330930 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:08,809 : INFO : EPOCH 1 - PROGRESS: at 45.23% examples, 330627 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-10 14:12:09,811 : INFO : EPOCH 1 - PROGRESS: at 47.84% examples, 330892 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:10,858 : INFO : EPOCH 1 - PROGRESS: at 50.45% examples, 330489 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-10 14:12:11,862 : INFO : EPOCH 1 - PROGRESS: at 53.10% examples, 330583 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:12,867 : INFO : EPOCH 1 - PROGRESS: at 55.77% examples, 331098 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:13,872 : INFO : EPOCH 1 - PROGRESS: at 58.24% examples, 330574 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:14,897 : INFO : EPOCH 1 - PROGRESS: at 60.90% examples, 330771 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:15,928 : INFO : EPOCH 1 - PROGRESS: at 63.62% examples, 330869 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:16,978 : INFO : EPOCH 1 - PROGRESS: at 66.33% examples, 330998 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:18,010 : INFO : EPOCH 1 - PROGRESS: at 69.03% examples, 331067 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:19,048 : INFO : EPOCH 1 - PROGRESS: at 71.76% examples, 331325 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:20,057 : INFO : EPOCH 1 - PROGRESS: at 74.51% examples, 331890 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:21,066 : INFO : EPOCH 1 - PROGRESS: at 77.10% examples, 331680 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:22,104 : INFO : EPOCH 1 - PROGRESS: at 79.75% examples, 331426 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:23,108 : INFO : EPOCH 1 - PROGRESS: at 82.44% examples, 331756 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:24,127 : INFO : EPOCH 1 - PROGRESS: at 84.94% examples, 331284 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:25,144 : INFO : EPOCH 1 - PROGRESS: at 87.59% examples, 331256 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:26,173 : INFO : EPOCH 1 - PROGRESS: at 90.31% examples, 331525 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:27,198 : INFO : EPOCH 1 - PROGRESS: at 93.01% examples, 331628 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:28,207 : INFO : EPOCH 1 - PROGRESS: at 95.76% examples, 331861 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:29,213 : INFO : EPOCH 1 - PROGRESS: at 98.34% examples, 331933 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:29,763 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-10 14:12:29,784 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-10 14:12:29,802 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-10 14:12:29,806 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-10 14:12:29,808 : INFO : EPOCH - 1 : training on 17798082 raw words (12751271 effective words) took 38.4s, 332372 effective words/s\n",
            "2020-01-10 14:12:30,821 : INFO : EPOCH 2 - PROGRESS: at 2.50% examples, 321241 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-10 14:12:31,833 : INFO : EPOCH 2 - PROGRESS: at 5.08% examples, 322982 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:32,834 : INFO : EPOCH 2 - PROGRESS: at 7.77% examples, 327119 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:33,857 : INFO : EPOCH 2 - PROGRESS: at 10.47% examples, 329301 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:34,875 : INFO : EPOCH 2 - PROGRESS: at 13.14% examples, 329589 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:35,880 : INFO : EPOCH 2 - PROGRESS: at 15.79% examples, 330387 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:36,904 : INFO : EPOCH 2 - PROGRESS: at 18.52% examples, 330993 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:37,932 : INFO : EPOCH 2 - PROGRESS: at 21.23% examples, 331391 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:38,996 : INFO : EPOCH 2 - PROGRESS: at 24.04% examples, 332015 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-10 14:12:40,030 : INFO : EPOCH 2 - PROGRESS: at 26.69% examples, 331285 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:41,037 : INFO : EPOCH 2 - PROGRESS: at 29.31% examples, 331490 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:42,063 : INFO : EPOCH 2 - PROGRESS: at 32.02% examples, 331126 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:43,085 : INFO : EPOCH 2 - PROGRESS: at 34.70% examples, 331502 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-10 14:12:44,097 : INFO : EPOCH 2 - PROGRESS: at 37.34% examples, 331526 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:45,147 : INFO : EPOCH 2 - PROGRESS: at 40.01% examples, 331217 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:46,166 : INFO : EPOCH 2 - PROGRESS: at 42.69% examples, 331591 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-10 14:12:47,194 : INFO : EPOCH 2 - PROGRESS: at 45.46% examples, 332128 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:48,207 : INFO : EPOCH 2 - PROGRESS: at 48.01% examples, 331722 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:49,221 : INFO : EPOCH 2 - PROGRESS: at 50.62% examples, 331731 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:50,259 : INFO : EPOCH 2 - PROGRESS: at 53.38% examples, 332021 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:51,290 : INFO : EPOCH 2 - PROGRESS: at 56.00% examples, 331733 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:52,338 : INFO : EPOCH 2 - PROGRESS: at 58.68% examples, 331837 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-10 14:12:53,352 : INFO : EPOCH 2 - PROGRESS: at 61.42% examples, 332413 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:54,358 : INFO : EPOCH 2 - PROGRESS: at 64.00% examples, 332184 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:55,388 : INFO : EPOCH 2 - PROGRESS: at 66.69% examples, 332225 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:56,394 : INFO : EPOCH 2 - PROGRESS: at 69.31% examples, 332304 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:57,426 : INFO : EPOCH 2 - PROGRESS: at 72.04% examples, 332569 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:12:58,465 : INFO : EPOCH 2 - PROGRESS: at 74.74% examples, 332511 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:12:59,479 : INFO : EPOCH 2 - PROGRESS: at 77.49% examples, 332949 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:00,549 : INFO : EPOCH 2 - PROGRESS: at 80.31% examples, 332990 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:01,575 : INFO : EPOCH 2 - PROGRESS: at 82.99% examples, 333041 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:02,589 : INFO : EPOCH 2 - PROGRESS: at 85.56% examples, 332778 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-10 14:13:03,600 : INFO : EPOCH 2 - PROGRESS: at 88.23% examples, 332981 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:04,618 : INFO : EPOCH 2 - PROGRESS: at 90.93% examples, 333105 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:05,620 : INFO : EPOCH 2 - PROGRESS: at 93.63% examples, 333373 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:06,646 : INFO : EPOCH 2 - PROGRESS: at 96.24% examples, 333022 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-10 14:13:07,680 : INFO : EPOCH 2 - PROGRESS: at 99.05% examples, 333559 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:07,978 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-10 14:13:07,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-10 14:13:08,018 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-10 14:13:08,026 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-10 14:13:08,027 : INFO : EPOCH - 2 : training on 17798082 raw words (12749554 effective words) took 38.2s, 333694 effective words/s\n",
            "2020-01-10 14:13:09,061 : INFO : EPOCH 3 - PROGRESS: at 2.56% examples, 323051 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:10,079 : INFO : EPOCH 3 - PROGRESS: at 5.19% examples, 325650 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:11,093 : INFO : EPOCH 3 - PROGRESS: at 7.94% examples, 329828 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:12,096 : INFO : EPOCH 3 - PROGRESS: at 10.59% examples, 331096 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:13,124 : INFO : EPOCH 3 - PROGRESS: at 13.31% examples, 331817 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:14,173 : INFO : EPOCH 3 - PROGRESS: at 16.06% examples, 332181 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:15,198 : INFO : EPOCH 3 - PROGRESS: at 18.86% examples, 333505 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:16,235 : INFO : EPOCH 3 - PROGRESS: at 21.52% examples, 332306 words/s, in_qsize 8, out_qsize 2\n",
            "2020-01-10 14:13:17,248 : INFO : EPOCH 3 - PROGRESS: at 24.27% examples, 333852 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:18,260 : INFO : EPOCH 3 - PROGRESS: at 27.03% examples, 335016 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:19,271 : INFO : EPOCH 3 - PROGRESS: at 29.64% examples, 334781 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:20,310 : INFO : EPOCH 3 - PROGRESS: at 32.57% examples, 336115 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:21,351 : INFO : EPOCH 3 - PROGRESS: at 35.27% examples, 335603 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:22,356 : INFO : EPOCH 3 - PROGRESS: at 37.90% examples, 335515 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:23,395 : INFO : EPOCH 3 - PROGRESS: at 40.69% examples, 336091 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:24,424 : INFO : EPOCH 3 - PROGRESS: at 43.31% examples, 335498 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:25,430 : INFO : EPOCH 3 - PROGRESS: at 45.94% examples, 335423 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:26,440 : INFO : EPOCH 3 - PROGRESS: at 48.66% examples, 336048 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:27,455 : INFO : EPOCH 3 - PROGRESS: at 51.31% examples, 335803 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:28,459 : INFO : EPOCH 3 - PROGRESS: at 53.99% examples, 336071 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:29,487 : INFO : EPOCH 3 - PROGRESS: at 56.65% examples, 335934 words/s, in_qsize 5, out_qsize 2\n",
            "2020-01-10 14:13:30,524 : INFO : EPOCH 3 - PROGRESS: at 59.39% examples, 336345 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:31,545 : INFO : EPOCH 3 - PROGRESS: at 62.13% examples, 336628 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:32,594 : INFO : EPOCH 3 - PROGRESS: at 64.79% examples, 335948 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-10 14:13:33,604 : INFO : EPOCH 3 - PROGRESS: at 67.51% examples, 336378 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:34,625 : INFO : EPOCH 3 - PROGRESS: at 70.10% examples, 335849 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:35,646 : INFO : EPOCH 3 - PROGRESS: at 72.72% examples, 335616 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:36,649 : INFO : EPOCH 3 - PROGRESS: at 75.30% examples, 335340 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:37,666 : INFO : EPOCH 3 - PROGRESS: at 78.05% examples, 335656 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:38,687 : INFO : EPOCH 3 - PROGRESS: at 80.64% examples, 335197 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:39,707 : INFO : EPOCH 3 - PROGRESS: at 83.28% examples, 335033 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:40,722 : INFO : EPOCH 3 - PROGRESS: at 85.96% examples, 335128 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:41,752 : INFO : EPOCH 3 - PROGRESS: at 88.63% examples, 335056 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:42,764 : INFO : EPOCH 3 - PROGRESS: at 91.37% examples, 335381 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:43,787 : INFO : EPOCH 3 - PROGRESS: at 94.03% examples, 335203 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:44,809 : INFO : EPOCH 3 - PROGRESS: at 96.75% examples, 335223 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:45,817 : INFO : EPOCH 3 - PROGRESS: at 99.38% examples, 335377 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:46,005 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-10 14:13:46,007 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-10 14:13:46,021 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-10 14:13:46,038 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-10 14:13:46,040 : INFO : EPOCH - 3 : training on 17798082 raw words (12748343 effective words) took 38.0s, 335460 effective words/s\n",
            "2020-01-10 14:13:47,077 : INFO : EPOCH 4 - PROGRESS: at 2.56% examples, 321783 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:48,120 : INFO : EPOCH 4 - PROGRESS: at 5.30% examples, 328966 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:49,140 : INFO : EPOCH 4 - PROGRESS: at 8.06% examples, 331969 words/s, in_qsize 8, out_qsize 2\n",
            "2020-01-10 14:13:50,137 : INFO : EPOCH 4 - PROGRESS: at 10.87% examples, 337985 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-10 14:13:51,162 : INFO : EPOCH 4 - PROGRESS: at 13.48% examples, 334702 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:52,173 : INFO : EPOCH 4 - PROGRESS: at 16.23% examples, 336720 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:53,173 : INFO : EPOCH 4 - PROGRESS: at 18.80% examples, 334560 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-10 14:13:54,213 : INFO : EPOCH 4 - PROGRESS: at 21.58% examples, 334843 words/s, in_qsize 7, out_qsize 2\n",
            "2020-01-10 14:13:55,289 : INFO : EPOCH 4 - PROGRESS: at 24.44% examples, 335377 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:13:56,308 : INFO : EPOCH 4 - PROGRESS: at 27.20% examples, 336154 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:57,309 : INFO : EPOCH 4 - PROGRESS: at 29.82% examples, 336120 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:58,337 : INFO : EPOCH 4 - PROGRESS: at 32.57% examples, 335862 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:13:59,347 : INFO : EPOCH 4 - PROGRESS: at 35.27% examples, 336134 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:00,353 : INFO : EPOCH 4 - PROGRESS: at 37.90% examples, 336002 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:01,393 : INFO : EPOCH 4 - PROGRESS: at 40.58% examples, 335615 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:02,405 : INFO : EPOCH 4 - PROGRESS: at 43.19% examples, 335427 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:03,407 : INFO : EPOCH 4 - PROGRESS: at 45.77% examples, 334994 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:04,414 : INFO : EPOCH 4 - PROGRESS: at 48.37% examples, 334934 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:05,443 : INFO : EPOCH 4 - PROGRESS: at 50.95% examples, 334163 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-10 14:14:06,445 : INFO : EPOCH 4 - PROGRESS: at 53.66% examples, 334561 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:07,465 : INFO : EPOCH 4 - PROGRESS: at 56.22% examples, 333967 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:08,474 : INFO : EPOCH 4 - PROGRESS: at 58.90% examples, 334543 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:09,491 : INFO : EPOCH 4 - PROGRESS: at 61.53% examples, 334372 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:10,507 : INFO : EPOCH 4 - PROGRESS: at 64.06% examples, 333649 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:11,514 : INFO : EPOCH 4 - PROGRESS: at 66.63% examples, 333376 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:12,531 : INFO : EPOCH 4 - PROGRESS: at 69.31% examples, 333542 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:13,582 : INFO : EPOCH 4 - PROGRESS: at 71.99% examples, 333437 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-10 14:14:14,581 : INFO : EPOCH 4 - PROGRESS: at 74.68% examples, 333640 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:15,598 : INFO : EPOCH 4 - PROGRESS: at 77.38% examples, 333761 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:16,651 : INFO : EPOCH 4 - PROGRESS: at 80.14% examples, 333723 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:17,699 : INFO : EPOCH 4 - PROGRESS: at 82.84% examples, 333523 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:18,705 : INFO : EPOCH 4 - PROGRESS: at 85.51% examples, 333754 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:19,719 : INFO : EPOCH 4 - PROGRESS: at 88.18% examples, 333890 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:20,744 : INFO : EPOCH 4 - PROGRESS: at 90.82% examples, 333713 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:21,779 : INFO : EPOCH 4 - PROGRESS: at 93.57% examples, 333860 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:22,785 : INFO : EPOCH 4 - PROGRESS: at 96.30% examples, 334077 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:23,785 : INFO : EPOCH 4 - PROGRESS: at 98.89% examples, 334134 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:24,128 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-10 14:14:24,173 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-10 14:14:24,183 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-10 14:14:24,196 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-10 14:14:24,198 : INFO : EPOCH - 4 : training on 17798082 raw words (12749440 effective words) took 38.1s, 334247 effective words/s\n",
            "2020-01-10 14:14:25,222 : INFO : EPOCH 5 - PROGRESS: at 2.62% examples, 331682 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:26,233 : INFO : EPOCH 5 - PROGRESS: at 5.19% examples, 328415 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:27,274 : INFO : EPOCH 5 - PROGRESS: at 7.94% examples, 328785 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:28,277 : INFO : EPOCH 5 - PROGRESS: at 10.64% examples, 332129 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-10 14:14:29,278 : INFO : EPOCH 5 - PROGRESS: at 13.25% examples, 331553 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:30,297 : INFO : EPOCH 5 - PROGRESS: at 15.95% examples, 332426 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:31,308 : INFO : EPOCH 5 - PROGRESS: at 18.63% examples, 332310 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-10 14:14:32,321 : INFO : EPOCH 5 - PROGRESS: at 21.30% examples, 332259 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:33,346 : INFO : EPOCH 5 - PROGRESS: at 23.93% examples, 331809 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:34,358 : INFO : EPOCH 5 - PROGRESS: at 26.51% examples, 331102 words/s, in_qsize 4, out_qsize 3\n",
            "2020-01-10 14:14:35,359 : INFO : EPOCH 5 - PROGRESS: at 29.20% examples, 332111 words/s, in_qsize 8, out_qsize 0\n",
            "2020-01-10 14:14:36,387 : INFO : EPOCH 5 - PROGRESS: at 31.90% examples, 331665 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:37,389 : INFO : EPOCH 5 - PROGRESS: at 34.49% examples, 331347 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:38,391 : INFO : EPOCH 5 - PROGRESS: at 37.11% examples, 331617 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:39,425 : INFO : EPOCH 5 - PROGRESS: at 39.81% examples, 331656 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:40,457 : INFO : EPOCH 5 - PROGRESS: at 42.35% examples, 330848 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-10 14:14:41,469 : INFO : EPOCH 5 - PROGRESS: at 45.00% examples, 330951 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:42,498 : INFO : EPOCH 5 - PROGRESS: at 47.62% examples, 330685 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:43,506 : INFO : EPOCH 5 - PROGRESS: at 50.22% examples, 330834 words/s, in_qsize 7, out_qsize 1\n",
            "2020-01-10 14:14:44,508 : INFO : EPOCH 5 - PROGRESS: at 52.95% examples, 331429 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:45,513 : INFO : EPOCH 5 - PROGRESS: at 55.56% examples, 331548 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:46,527 : INFO : EPOCH 5 - PROGRESS: at 58.18% examples, 331835 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:47,548 : INFO : EPOCH 5 - PROGRESS: at 60.75% examples, 331426 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:48,562 : INFO : EPOCH 5 - PROGRESS: at 63.45% examples, 331708 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:49,592 : INFO : EPOCH 5 - PROGRESS: at 66.06% examples, 331485 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:50,602 : INFO : EPOCH 5 - PROGRESS: at 68.78% examples, 331798 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:51,649 : INFO : EPOCH 5 - PROGRESS: at 71.33% examples, 331132 words/s, in_qsize 8, out_qsize 1\n",
            "2020-01-10 14:14:52,651 : INFO : EPOCH 5 - PROGRESS: at 73.96% examples, 331292 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:53,659 : INFO : EPOCH 5 - PROGRESS: at 76.55% examples, 331124 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:54,684 : INFO : EPOCH 5 - PROGRESS: at 79.22% examples, 331272 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:55,698 : INFO : EPOCH 5 - PROGRESS: at 81.86% examples, 331272 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:56,758 : INFO : EPOCH 5 - PROGRESS: at 84.61% examples, 331254 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:57,794 : INFO : EPOCH 5 - PROGRESS: at 87.31% examples, 331251 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:14:58,845 : INFO : EPOCH 5 - PROGRESS: at 89.97% examples, 331110 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:14:59,873 : INFO : EPOCH 5 - PROGRESS: at 92.68% examples, 331204 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:15:00,886 : INFO : EPOCH 5 - PROGRESS: at 95.35% examples, 331223 words/s, in_qsize 6, out_qsize 1\n",
            "2020-01-10 14:15:01,927 : INFO : EPOCH 5 - PROGRESS: at 98.06% examples, 331383 words/s, in_qsize 7, out_qsize 0\n",
            "2020-01-10 14:15:02,636 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2020-01-10 14:15:02,644 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2020-01-10 14:15:02,647 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2020-01-10 14:15:02,653 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2020-01-10 14:15:02,655 : INFO : EPOCH - 5 : training on 17798082 raw words (12749139 effective words) took 38.4s, 331604 effective words/s\n",
            "2020-01-10 14:15:02,657 : INFO : training on a 88990410 raw words (63747747 effective words) took 191.2s, 333369 effective words/s\n",
            "2020-01-10 14:15:02,659 : INFO : precomputing L2-norms of word weight vectors\n",
            "2020-01-10 14:15:02,808 : INFO : saving Word2Vec object under /content/drive/My Drive/ML_datasets/300features_40minwords_10context, separately None\n",
            "2020-01-10 14:15:02,809 : INFO : not storing attribute vectors_norm\n",
            "2020-01-10 14:15:02,812 : INFO : not storing attribute cum_table\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-01-10 14:15:03,531 : INFO : saved /content/drive/My Drive/ML_datasets/300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIDq0ClC05OU",
        "colab_type": "code",
        "outputId": "ba66033f-7e8a-4118-b420-ab2f3c5c4671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "model.doesnt_match(\"france england germany berlin\".split())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'berlin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAkoUica1mh0",
        "colab_type": "code",
        "outputId": "92b4b9d9-797c-43ee-cc88-f9d4df9ba4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "model.doesnt_match(\"paris berlin london austria\".split())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'paris'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2yBF8UM1vmT",
        "colab_type": "code",
        "outputId": "ef1e276c-37aa-4d3a-f74e-0d5aba7abd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "model.most_similar(\"happy\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('satisfied', 0.535423219203949),\n",
              " ('afraid', 0.45269709825515747),\n",
              " ('lucky', 0.44025880098342896),\n",
              " ('pleased', 0.43810075521469116),\n",
              " ('grateful', 0.4347759187221527),\n",
              " ('proud', 0.4189411699771881),\n",
              " ('ready', 0.4110080599784851),\n",
              " ('thankful', 0.4036791920661926),\n",
              " ('happier', 0.40216904878616333),\n",
              " ('glad', 0.4007124900817871)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoIvBJKj2U80",
        "colab_type": "code",
        "outputId": "a82db309-32fb-4fc1-ec0b-c37e38c65b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "model.similarity(\"happy\",\"beautiful\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10380092"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu3GXh4A525W",
        "colab_type": "code",
        "outputId": "6d76f71a-e51e-48f9-cf3b-be7d71a6782e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec.load(\"/content/drive/My Drive/ML_datasets/300features_40minwords_10context\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-01-10 14:47:19,553 : INFO : loading Word2Vec object from /content/drive/My Drive/ML_datasets/300features_40minwords_10context\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "2020-01-10 14:47:20,016 : INFO : loading wv recursively from /content/drive/My Drive/ML_datasets/300features_40minwords_10context.wv.* with mmap=None\n",
            "2020-01-10 14:47:20,017 : INFO : setting ignored attribute vectors_norm to None\n",
            "2020-01-10 14:47:20,018 : INFO : loading vocabulary recursively from /content/drive/My Drive/ML_datasets/300features_40minwords_10context.vocabulary.* with mmap=None\n",
            "2020-01-10 14:47:20,021 : INFO : loading trainables recursively from /content/drive/My Drive/ML_datasets/300features_40minwords_10context.trainables.* with mmap=None\n",
            "2020-01-10 14:47:20,022 : INFO : setting ignored attribute cum_table to None\n",
            "2020-01-10 14:47:20,023 : INFO : loaded /content/drive/My Drive/ML_datasets/300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIe-teOj7I69",
        "colab_type": "code",
        "outputId": "765fb12e-4fe4-45d0-cece-7680f53aacc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "model.syn0_lockf.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0_lockf` (Attribute will be removed in 4.0.0, use self.trainables.vectors_lockf instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16490,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    }
  ]
}